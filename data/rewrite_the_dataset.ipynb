{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'akhil']\n",
      "0.709297266606\n"
     ]
    }
   ],
   "source": [
    "# This Source Code Form is subject to the terms of the MPL\n",
    "# License. If a copy of the same was not distributed with this\n",
    "# file, You can obtain one at\n",
    "# https://github.com/AkhilHector/pubundsci/blob/master/LICENSE.\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt, log\n",
    "from itertools import groupby\n",
    "from nltk.collocations import *\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from itertools import chain, product\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize as tokenize\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\"\"\"\n",
    "m1 - The number of all word forms a text consists\n",
    "m2 - The sum of the products of each observed frequency to the power of two\n",
    "     and the number of word types observed with that frequency\n",
    "\"\"\"\n",
    "\n",
    "def compute_average_word_length(sentence):\n",
    "    return np.mean([len(words) for words in sentence.split()])\n",
    "\n",
    "def compute_average_sentence_length(sentence):\n",
    "    sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    return np.mean([len(words) for words in sentence])\n",
    "\n",
    "def freq_of_words_great_sent_len(sentence):\n",
    "    result = []\n",
    "    avg_word_len = compute_average_word_length(sentence)\n",
    "    # sentence = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", sentence)\n",
    "    sentence = Counter(sentence.split())\n",
    "    for key, value in sentence.items():\n",
    "        if len(key) > avg_word_len:\n",
    "            result.append(value)\n",
    "#             print (key, value)\n",
    "    return sum(result)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.split(r\"[^0-9A-Za-z\\-'_]+\", sentence)\n",
    "\n",
    "def compute_yules_k_for_text(sentence):\n",
    "    tokens = tokenize(sentence)\n",
    "    counter = Counter(token.upper() for token in tokens)\n",
    "\n",
    "    #compute number of word forms in a given sentence/text\n",
    "    m1 = sum(counter.values())\n",
    "    m2 = sum([frequency ** 2 for frequency in counter.values()])\n",
    "\n",
    "    #compute yules k measure and return the value\n",
    "    yules_k = 10000/((m1 * m1) / (m2 - m1))\n",
    "    return yules_k\n",
    "\n",
    "\n",
    "def words_in_sentence(sentence):\n",
    "    w = [words.strip(\"0123456789!:,.?()[]{}\") for words in sentence.split()]\n",
    "    return filter(lambda x: len(x) > 0, w)\n",
    "\n",
    "def compute_yules_i_for_text(sentence):\n",
    "    dictionary = {}\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in words_in_sentence(sentence):\n",
    "        word = stemmer.stem(word).lower()\n",
    "        try:\n",
    "            dictionary[word] += 1\n",
    "        except:\n",
    "            dictionary[word] = 1\n",
    "\n",
    "    m1 = float(len(dictionary))\n",
    "    m2 = sum([len(list(grouped_values)) * (frequency ** 2) for frequency, grouped_values in groupby(sorted(dictionary.values()))])\n",
    "\n",
    "    # compute yules i and return the value\n",
    "    try:\n",
    "        yules_i = (m1 * m1) / (m2 - m1)\n",
    "        return yules_i\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "\n",
    "def compute_collocation_score(sentence_one, sentence_two, option):\n",
    "    if option == \"bi\":\n",
    "        tokens_for_one = nltk.wordpunct_tokenize(sentence_one)\n",
    "        tokens_for_two = nltk.wordpunct_tokenize(sentence_two)\n",
    "        finder_one = BigramCollocationFinder.from_words(tokens_for_one)\n",
    "        finder_two = BigramCollocationFinder.from_words(tokens_for_two)\n",
    "        result_one = finder_one.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq)\n",
    "        result_one = [(tuple(map(str.lower, values)), scores) for values, scores in result_one]\n",
    "        result_two = finder_two.score_ngrams(nltk.collocations.BigramAssocMeasures().raw_freq)\n",
    "        result_two = [(tuple(map(str.lower, values)), scores) for values, scores in result_two]\n",
    "        matches = [keys for keys in set(result_one).intersection(set(result_two))]\n",
    "        return len(matches)\n",
    "    elif option == \"tri\":\n",
    "        tokens_for_one = nltk.wordpunct_tokenize(sentence_one)\n",
    "        tokens_for_two = nltk.wordpunct_tokenize(sentence_two)\n",
    "        finder_one = TrigramCollocationFinder.from_words(tokens_for_one)\n",
    "        finder_two = TrigramCollocationFinder.from_words(tokens_for_two)\n",
    "        result_one = finder_one.score_ngrams(nltk.collocations.TrigramAssocMeasures().raw_freq)\n",
    "        result_one = [(tuple(map(str.lower, values)), scores) for values, scores in result_one]\n",
    "        result_two = finder_two.score_ngrams(nltk.collocations.TrigramAssocMeasures().raw_freq)\n",
    "        result_two = [(tuple(map(str.lower, values)), scores) for values, scores in result_two]\n",
    "        matches = [keys for keys in set(result_one).intersection(set(result_two))]\n",
    "        return len(matches)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def cosine_sim(sentence_a, sentence_b):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(tuple([' '.join(remove_stopwords(sentence_a))]) + tuple([' '.join(remove_stopwords(sentence_b))]))\n",
    "    return cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)[0][1]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stemmer =  PorterStemmer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in [stemmer.stem(words) for words in tokenize(text)] if w.lower() not in stopwords]\n",
    "    return content\n",
    "\n",
    "def text_arrangement(text):\n",
    "    result = []\n",
    "    stemmer =  PorterStemmer()\n",
    "    count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    \n",
    "    for words in text.split():\n",
    "        result.append(stemmer.stem(words))\n",
    "    count_vectorizer.fit(result)\n",
    "    return count_vectorizer.transform(result)\n",
    "    \n",
    "a = \"my name is akhil\"\n",
    "b = \"my name is akhil pandey\"\n",
    "print(remove_stopwords(a))\n",
    "print(cosine_sim(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the dataframe using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"DumpBlogs.csv\")\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)\n",
    "raw_data = raw_data.dropna(axis=0, how='any')\n",
    "raw_data = raw_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine sim score on Abs and blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.assign(similarity_score = [cosine_sim(raw_data[\"abstract\"][each], raw_data[\"blog_post\"][each]) for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "# raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yules I Measure on Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.assign(yules_i_for_abs = [compute_yules_i_for_text(raw_data[\"abstract\"][each]) for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "# raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yules I Measure on Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.assign(yules_i_for_blg = [compute_yules_i_for_text(raw_data[\"blog_post\"][each]) for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "# raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average word length on Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.assign(avg_word_len_abs = [compute_average_word_length(raw_data[\"abstract\"][each]) for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "# raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average sentence length on Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.assign(avg_sen_len_abs = [compute_average_sentence_length(raw_data[\"abstract\"][each]) for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "# raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency of words greater than avg word length in Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.assign(freq_of_words_great_sent_len_abs = [freq_of_words_great_sent_len(raw_data[\"abstract\"][each]) for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "# raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average word length on Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "raw_data = raw_data.assign(avg_word_len_blg = [compute_average_word_length(raw_data[\"blog_post\"][each]) for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "# raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average sentence length on Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.assign(avg_sen_len_blg = [compute_average_sentence_length(raw_data[\"blog_post\"][each]) for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "# raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency of words greater than avg word length in Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.assign(freq_of_words_great_sent_len_blg = [freq_of_words_great_sent_len(raw_data[\"blog_post\"][each]) for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "# raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams between abstract and blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data = raw_data.assign(bigrams_abs_blg = [compute_collocation_score(\" \".join(remove_stopwords(raw_data[\"abstract\"][each])), \" \".join(remove_stopwords(raw_data[\"blog_post\"][each])), \"bi\") for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "# raw_data.bigrams_abs_blg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trigrams between abstract and blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1, 20,  5, 26,  4,  2, 10, 23,  3, 18, 17, 24, 30, 19, 22, 11,\n",
       "        7,  8,  6, 15, 25, 16, 13, 27, 12, 21, 29, 14,  9], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = raw_data.assign(trigrams_abs_blg = [compute_collocation_score(\" \".join(remove_stopwords(raw_data[\"abstract\"][each])), \" \".join(remove_stopwords(raw_data[\"blog_post\"][each])), \"tri\") for each in range(0, len(raw_data[\"altmetric_id\"]))])\n",
    "raw_data.bigrams_abs_blg.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the Dataset to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data.to_csv(\"blogs_score_beta.csv\", sep = ',', encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
